# pyspider
知乎爬虫和v2ex爬虫的实现。使用python的pyspider爬虫进行开发，主要爬取知乎的问题和评论，以及v2ex的帖子。数据转储到mysql数据库，用于zhihu项目的使用。


## 使用python爬虫爬取知乎和v2ex数据，充实网站信息

	安装python2.7并且配置环境变量。同时安装pycharm，配置interpretor，安装pip。

	这里会各种报错，主要是中文目录以及pip版本导致的错误，需要修改各种配置文件以支持gbk编码。详情略。

	安装好以后，我们先熟悉一下python的语法，写一些例子，比如数据类型，操作符，方法调用，以及面向对象的技术。

	因为数据是要导入数据库的，所以这里安装MySQLdb的一个库，并且写一下连接数据库的代码，写一下简单的crud进行测试。

	使用requests库作为解析http请求的工具，使用beautifulsoup作为解析html代码的工具，请求之后直接使用css选择器匹配。即可获得内容。

	当然现在我们有更方便的工具pyspider，可以方便解析请求并且可以设置代理，伪装身份等，直接传入url并且写好多级的解析函数，程序便会迭代执行，直到把所有页面的内容解析出来。这里我们直接启动pyspider的web应用并且写好python代码，就可以执行爬虫了。简单讲一下知乎和v2ex的爬虫流程。

	v2ex：
	首先请求首页，因为v2ex现在也是https页面了，所以需要默认把使用证书设为false。

	执行完index_page函数，说明首页已经请求完毕，我们了解其css布局和url特征以后，根据tab=?可以进入下一级分类。于是for循环爬出所有以tab=?结尾的url，并且分别请求，进入下一级函数。

	根据页面的层级和css格式我们设置好多级函数依次循环执行，这样我们就可以解析到最后一级真正的帖子内容了。

	同理，知乎也是这样，先找到问题，再把问题下所有的回答进行爬取，最后把问题和评论一起处理。

	当然最后一级内容需要调用数据库的存储接口，为了避免存储错误，需要把内容中的 " 改成 //，否则会出问题。